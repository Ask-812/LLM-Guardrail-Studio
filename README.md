# LLM Guardrail Studio

A modular trust layer for local LLMs that provides comprehensive safety and moderation capabilities for open-source language models.

![Python](https://img.shields.io/badge/python-3.8+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)
![Tests](https://img.shields.io/badge/tests-passing-brightgreen.svg)

## ðŸš€ Overview

LLM Guardrail Studio is a production-ready safety pipeline designed to enhance the reliability of local LLM deployments. It provides real-time detection of hallucinations, toxicity, and prompt-response mismatches through an intuitive dashboard and robust API.

### âœ¨ Key Features

- **ðŸ›¡ï¸ Multi-Modal Safety Checks**: Toxicity detection, hallucination identification, and semantic alignment verification
- **ðŸ“Š Interactive Dashboard**: Real-time Streamlit interface with analytics, history tracking, and batch processing
- **ðŸ”§ Modular Architecture**: Easily extensible with custom evaluation modules
- **ðŸ¤– Multi-Model Support**: Compatible with Mistral, Zephyr, Llama 2, and other open-source LLMs
- **âš¡ Production Ready**: Comprehensive testing, logging, and deployment options
- **ðŸ“ˆ Advanced Analytics**: Trend analysis, score distributions, and detailed reporting

## ðŸŽ¯ Use Cases

- **Content Moderation**: Filter harmful or inappropriate AI-generated content
- **Quality Assurance**: Ensure AI responses meet quality and relevance standards
- **Compliance Monitoring**: Track and audit AI system outputs for regulatory compliance
- **Research & Development**: Analyze model behavior and performance across different scenarios

## ðŸ“¦ Installation

### Quick Start

```bash
# Clone the repository
git clone https://github.com/Ask-812/LLM-Guardrail-Studio.git
cd LLM-Guardrail-Studio

# Install dependencies
pip install -r requirements.txt

# Launch dashboard
streamlit run app.py
```

### Development Setup

```bash
# Install with development dependencies
make install-dev

# Run tests
make test

# Format code
make format
```

## ðŸš€ Quick Start

### Basic Usage

```python
from guardrails import GuardrailPipeline

# Initialize the pipeline
pipeline = GuardrailPipeline(
    enable_toxicity=True,
    enable_hallucination=True,
    enable_alignment=True
)

# Evaluate a response
result = pipeline.evaluate(
    prompt="What is the capital of France?",
    response="The capital of France is Paris."
)

print(f"âœ… Passed: {result.passed}")
print(f"ðŸ“Š Scores: {result.scores}")
print(f"ðŸš© Flags: {result.flags}")
```

### Dashboard

Launch the interactive dashboard:

```bash
streamlit run app.py
```

Features include:
- Real-time evaluation interface
- Batch processing capabilities
- Analytics and trend visualization
- Evaluation history tracking
- Downloadable reports

### Integration with Local Models

```python
from models import LLMWrapper
from guardrails import GuardrailPipeline

# Initialize model and guardrails
model = LLMWrapper("microsoft/phi-2")
pipeline = GuardrailPipeline()

# Generate and evaluate
prompt = "Explain quantum computing"
response = model.generate(prompt)
result = pipeline.evaluate(prompt, response)
```

## ðŸ“Š Evaluation Metrics

### Toxicity Detection
- **Range**: 0-1 (lower is better)
- **Threshold**: Configurable (default: 0.7)
- **Technology**: Detoxify transformer models

### Semantic Alignment
- **Range**: -1 to 1 (higher is better)
- **Threshold**: Configurable (default: 0.5)
- **Technology**: SentenceTransformers cosine similarity

### Hallucination Risk
- **Range**: 0-1 (lower is better)
- **Threshold**: Configurable (default: 0.6)
- **Technology**: Uncertainty detection and confidence analysis

## ðŸ—ï¸ Architecture

```
llm-guardrail-studio/
â”œâ”€â”€ guardrails/          # Core evaluation modules
â”‚   â”œâ”€â”€ evaluators.py    # Individual safety evaluators
â”‚   â”œâ”€â”€ pipeline.py      # Main orchestration pipeline
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ models/              # Model wrappers and utilities
â”‚   â”œâ”€â”€ llm_wrapper.py   # Local LLM integration
â”‚   â”œâ”€â”€ model_loader.py  # Model management utilities
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ dashboard/           # Streamlit dashboard components
â”‚   â”œâ”€â”€ components.py    # Reusable UI components
â”‚   â”œâ”€â”€ utils.py         # Dashboard utilities
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ tests/               # Comprehensive test suite
â”œâ”€â”€ examples/            # Usage examples and tutorials
â”œâ”€â”€ docs/                # Documentation
â””â”€â”€ app.py              # Main dashboard application
```

## ðŸ”§ Configuration

### Environment Variables

```bash
# Model configuration
export GUARDRAIL_MODEL_NAME=mistralai/Mistral-7B-v0.1
export GUARDRAIL_DEVICE=cuda

# Safety thresholds
export GUARDRAIL_TOXICITY_THRESHOLD=0.7
export GUARDRAIL_ALIGNMENT_THRESHOLD=0.5
export GUARDRAIL_HALLUCINATION_THRESHOLD=0.6

# Performance settings
export GUARDRAIL_BATCH_SIZE=32
export GUARDRAIL_MAX_LENGTH=512
```

### Custom Evaluators

Extend the system with custom evaluators:

```python
class CustomEvaluator:
    def evaluate(self, text: str) -> float:
        # Your custom evaluation logic
        return score

# Add to pipeline
pipeline.evaluators['custom'] = CustomEvaluator()
```

## ðŸ“ˆ Performance

- **Throughput**: 100+ evaluations/second (CPU), 500+ evaluations/second (GPU)
- **Latency**: <100ms per evaluation (excluding model inference)
- **Memory**: 2-4GB RAM (depending on models loaded)
- **Scalability**: Horizontal scaling supported via API deployment

## ðŸ§ª Testing

```bash
# Run all tests
make test

# Run specific test modules
python -m pytest tests/test_evaluators.py -v
python -m pytest tests/test_pipeline.py -v

# Run with coverage
make test
```

## ðŸ“š Documentation

- **[API Documentation](docs/API.md)**: Comprehensive API reference
- **[Deployment Guide](docs/DEPLOYMENT.md)**: Production deployment options
- **[Contributing Guide](CONTRIBUTING.md)**: Development and contribution guidelines

## ðŸš€ Deployment

### Docker

```bash
docker build -t guardrail-studio .
docker run -p 8501:8501 guardrail-studio
```

### Cloud Platforms

- **Streamlit Cloud**: Direct GitHub integration
- **Heroku**: One-click deployment
- **AWS/GCP/Azure**: Container and serverless options

See [Deployment Guide](docs/DEPLOYMENT.md) for detailed instructions.

## ðŸ¤ Contributing

We welcome contributions! Please see our [Contributing Guide](CONTRIBUTING.md) for details on:

- Development setup
- Code style guidelines
- Testing requirements
- Pull request process

## ðŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ðŸ™ Acknowledgments

Built with these amazing open-source tools:

- **[Hugging Face Transformers](https://huggingface.co/transformers/)**: Model loading and inference
- **[SentenceTransformers](https://www.sbert.net/)**: Semantic similarity computation
- **[Detoxify](https://github.com/unitaryai/detoxify)**: Toxicity detection models
- **[Streamlit](https://streamlit.io/)**: Interactive dashboard framework
- **[Plotly](https://plotly.com/)**: Data visualization

## ðŸ“Š Project Stats

- **Language**: Python 3.8+
- **Dependencies**: 12 core packages
- **Test Coverage**: 90%+
- **Documentation**: Comprehensive API and deployment guides
- **Examples**: 10+ usage scenarios

## ðŸ”® Roadmap

- [ ] Advanced hallucination detection with fact-checking
- [ ] Multi-language support
- [ ] Custom model fine-tuning capabilities
- [ ] Enterprise SSO integration
- [ ] Advanced analytics and reporting
- [ ] Plugin system for third-party evaluators

---

**â­ Star this repository if you find it useful!**

For questions, issues, or feature requests, please [open an issue](https://github.com/Ask-812/LLM-Guardrail-Studio/issues).
