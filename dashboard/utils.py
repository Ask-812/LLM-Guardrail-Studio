"""
Utility functions for dashboard components
"""

from typing import Dict, Any
import streamlit as st


def format_score(score: float, metric_type: str) -> str:
    """Format score for display"""
    if metric_type == "percentage":
        return f"{score * 100:.1f}%"
    else:
        return f"{score:.3f}"


def get_status_color(score: float, metric_type: str) -> str:
    """Get color based on score and metric type"""
    if metric_type == "toxicity":
        if score > 0.7:
            return "red"
        elif score > 0.4:
            return "orange"
        else:
            return "green"
    elif metric_type == "alignment":
        if score > 0.7:
            return "green"
        elif score > 0.4:
            return "orange"
        else:
            return "red"
    elif metric_type == "hallucination_risk":
        if score > 0.6:
            return "red"
        elif score > 0.3:
            return "orange"
        else:
            return "green"
    else:
        return "blue"


def display_metric_explanation():
    """Display explanation of metrics"""
    with st.expander("ğŸ“Š Metric Explanations"):
        st.markdown("""
        **Toxicity Score**: Measures harmful or inappropriate content (0-1, lower is better)
        - ğŸŸ¢ < 0.4: Safe content
        - ğŸŸ¡ 0.4-0.7: Moderate concern
        - ğŸ”´ > 0.7: High toxicity detected
        
        **Alignment Score**: Semantic similarity between prompt and response (-1 to 1, higher is better)
        - ğŸ”´ < 0.4: Poor alignment
        - ğŸŸ¡ 0.4-0.7: Moderate alignment
        - ğŸŸ¢ > 0.7: Good alignment
        
        **Hallucination Risk**: Likelihood of factual errors or inconsistencies (0-1, lower is better)
        - ğŸŸ¢ < 0.3: Low risk
        - ğŸŸ¡ 0.3-0.6: Moderate risk
        - ğŸ”´ > 0.6: High risk
        """)


def create_download_report(evaluation_data: Dict[str, Any]) -> str:
    """Create downloadable evaluation report"""
    report = f"""
# LLM Guardrail Evaluation Report

## Input
**Prompt**: {evaluation_data.get('prompt', 'N/A')}
**Response**: {evaluation_data.get('response', 'N/A')}

## Evaluation Results
**Overall Status**: {'âœ… PASSED' if evaluation_data.get('passed', False) else 'âŒ FAILED'}

### Scores
"""
    
    scores = evaluation_data.get('scores', {})
    for metric, score in scores.items():
        report += f"- **{metric.replace('_', ' ').title()}**: {score:.3f}\n"
    
    flags = evaluation_data.get('flags', [])
    if flags:
        report += "\n### Issues Detected\n"
        for flag in flags:
            report += f"- âš ï¸ {flag}\n"
    
    report += f"\n---\n*Generated by LLM Guardrail Studio*"
    
    return report